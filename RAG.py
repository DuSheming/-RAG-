# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OI26MCKJF2AkZLans0O52OuNlmTd_XbC
"""

!pip install pandas selfies torch transformers deepchem rdkit swifter tqdm
!pip install faiss-cpu sentence-transformers pillow

!pip install rank_bm25 openai

"""
å¤šç´¢å¼•RAGç³»ç»Ÿ - å½»åº•è§£å†³å†…å­˜é—®é¢˜
ç­–ç•¥ï¼šæ„å»ºå¤šä¸ªç‹¬ç«‹çš„å°ç´¢å¼•ï¼Œé¿å…å•ä¸ªç´¢å¼•è¿‡å¤§
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict
import re
import os
import gc

import faiss
from sentence_transformers import SentenceTransformer
from tqdm import tqdm

# ============ é…ç½® ============
GDRIVE_BASE = "/content/drive/MyDrive"
PROJECT_NAME = "PatentRAG"
GDRIVE_PROJECT = f"{GDRIVE_BASE}/{PROJECT_NAME}"
os.makedirs(GDRIVE_PROJECT, exist_ok=True)

print(f"âœ“ é¡¹ç›®ç›®å½•: {GDRIVE_PROJECT}")


# ============ å¿«é€Ÿæ„å»ºå•ä¸ªå°ç´¢å¼• ============

def build_small_index(index_id: int, docs_per_index: int = 90):
    """
    æ„å»ºå•ä¸ªå°ç´¢å¼•ï¼ˆå®‰å…¨ï¼ï¼‰

    Args:
        index_id: ç´¢å¼•ç¼–å· (0-9)
        docs_per_index: æ¯ä¸ªç´¢å¼•åŒ…å«çš„æ–‡æ¡£æ•°
    """

    print(f"\n{'='*80}")
    print(f"ğŸ”¨ æ„å»ºç´¢å¼• {index_id}")
    print(f"{'='*80}")

    # 1. åŠ è½½æ¨¡å‹
    print("ğŸ”„ åŠ è½½æ¨¡å‹...")
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    dimension = 384

    # 2. æŸ¥æ‰¾æ–‡æ¡£
    base_path = Path("/content/drive/MyDrive/MinerU")
    all_docs = sorted([f.parent for f in base_path.rglob("full.md")])

    start_idx = index_id * docs_per_index
    end_idx = min(start_idx + docs_per_index, len(all_docs))

    if start_idx >= len(all_docs):
        print(f"âš ï¸  ç´¢å¼• {index_id} è¶…å‡ºèŒƒå›´")
        return

    my_docs = all_docs[start_idx:end_idx]
    print(f"ğŸ“ å¤„ç†æ–‡æ¡£ {start_idx+1}-{end_idx} ({len(my_docs)}ä¸ª)")

    # 3. åˆ›å»ºç´¢å¼•
    index = faiss.IndexFlatIP(dimension)
    documents = []

    # 4. å¤„ç†æ–‡æ¡£
    for doc_dir in tqdm(my_docs, desc=f"ç´¢å¼•{index_id}"):
        try:
            # è¯»å–full.md
            full_md = (doc_dir / "full.md").read_text(encoding='utf-8')

            # ç®€å•åˆ†å—ï¼ˆæŒ‰æ®µè½ï¼‰
            chunks = [p.strip() for p in full_md.split('\n\n') if len(p.strip()) > 50]

            if not chunks:
                continue

            # å‘é‡åŒ–ï¼ˆå°æ‰¹æ¬¡ï¼‰
            for i in range(0, len(chunks), 8):
                batch = chunks[i:i+8]
                embeddings = encoder.encode(batch, show_progress_bar=False, convert_to_numpy=True)
                embeddings = embeddings.astype('float32')
                faiss.normalize_L2(embeddings)

                index.add(embeddings)

                # ä¿å­˜å…ƒæ•°æ®
                for chunk in batch:
                    documents.append({
                        'content': chunk,
                        'metadata': {'doc_id': doc_dir.name, 'title': ''}
                    })
        except Exception as e:
            print(f"\nâœ— {doc_dir.name[:30]}: {str(e)[:50]}")

    # 5. ä¿å­˜è¿™ä¸ªå°ç´¢å¼•
    save_path = Path(f"{GDRIVE_PROJECT}/index_{index_id}")
    save_path.mkdir(parents=True, exist_ok=True)

    faiss.write_index(index, str(save_path / "faiss.index"))

    with open(save_path / "documents.pkl", 'wb') as f:
        pickle.dump({'documents': documents, 'dimension': dimension}, f)

    print(f"\nâœ… ç´¢å¼• {index_id} å®Œæˆ: {len(documents)} ä¸ªæ–‡æ¡£å—")
    print(f"ğŸ’¾ ä¿å­˜ä½ç½®: {save_path}")

    # æ¸…ç†å†…å­˜
    del encoder, index, documents
    gc.collect()


# ============ æŸ¥è¯¢å¤šä¸ªç´¢å¼• ============

def search_all_indices(query: str, top_k: int = 5):
    """æœç´¢æ‰€æœ‰å°ç´¢å¼•å¹¶åˆå¹¶ç»“æœ"""

    print(f"ğŸ” æœç´¢: {query}")

    # åŠ è½½æ¨¡å‹
    encoder = SentenceTransformer('all-MiniLM-L6-v2')
    query_embedding = encoder.encode(query, convert_to_numpy=True)
    query_embedding = query_embedding.astype('float32').reshape(1, -1)
    faiss.normalize_L2(query_embedding)

    all_results = []

    # æœç´¢æ¯ä¸ªç´¢å¼•
    for i in range(10):  # 0-9å…±10ä¸ªç´¢å¼•
        index_path = Path(f"{GDRIVE_PROJECT}/index_{i}")

        if not index_path.exists():
            continue

        # åŠ è½½ç´¢å¼•
        index = faiss.read_index(str(index_path / "faiss.index"))
        with open(index_path / "documents.pkl", 'rb') as f:
            data = pickle.load(f)

        # æœç´¢
        distances, indices = index.search(query_embedding, top_k)

        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(data['documents']):
                all_results.append({
                    'content': data['documents'][idx]['content'],
                    'metadata': data['documents'][idx]['metadata'],
                    'score': float(dist)
                })

    # æŒ‰åˆ†æ•°æ’åº
    all_results.sort(key=lambda x: x['score'], reverse=True)

    # è¿”å›Top-K
    return all_results[:top_k]


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

# æ­¥éª¤1: æ„å»ºæ‰€æœ‰å°ç´¢å¼•ï¼ˆä¸€æ¬¡ä¸€ä¸ªï¼Œé¿å…å†…å­˜é—®é¢˜ï¼‰
for i in range(10):
    build_small_index(i, docs_per_index=90)

# æ­¥éª¤2: æŸ¥è¯¢
results = search_all_indices("Which photoinitiators work better under LED?", top_k=5)

for i, r in enumerate(results, 1):
    print(f"\n[{i}] åˆ†æ•°: {r['score']:.3f}")
    print(f"å†…å®¹: {r['content'][:200]}...")

"""
å¤šç´¢å¼•é«˜çº§RAGç³»ç»Ÿ
åŠŸèƒ½ï¼šå¤šç´¢å¼• + BM25æ··åˆæ£€ç´¢ + é‡æ’åº + DeepSeek
"""

import json
import pickle
import numpy as np
from pathlib import Path
from typing import List, Dict
import re

import faiss
from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
from openai import OpenAI

# ============ é…ç½® ============
GDRIVE_PROJECT = "/content/drive/MyDrive/PatentRAG"
NUM_INDICES = 10  # ç´¢å¼•æ•°é‡


class MultiIndexRAG:
    """å¤šç´¢å¼•RAGç³»ç»Ÿï¼ˆæ”¯æŒé«˜çº§åŠŸèƒ½ï¼‰"""

    def __init__(self, use_reranker: bool = True, deepseek_api_key: str = None):
        """
        Args:
            use_reranker: æ˜¯å¦ä½¿ç”¨é‡æ’åº
            deepseek_api_key: DeepSeek APIå¯†é’¥ï¼ˆå¯é€‰ï¼‰
        """
        print("ğŸ”„ åˆå§‹åŒ–é«˜çº§RAGç³»ç»Ÿ...")

        # å‘é‡æ£€ç´¢æ¨¡å‹
        self.encoder = SentenceTransformer('all-MiniLM-L6-v2')

        # é‡æ’åºæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
        self.reranker = None
        if use_reranker:
            print("ğŸ”„ åŠ è½½é‡æ’åºæ¨¡å‹...")
            self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
            print("âœ“ é‡æ’åºæ¨¡å‹åŠ è½½å®Œæˆ")

        # DeepSeek LLMï¼ˆå¯é€‰ï¼‰
        self.llm = None
        if deepseek_api_key:
            self.llm = OpenAI(
                api_key=deepseek_api_key,
                base_url="https://api.deepseek.com"
            )
            print("âœ“ DeepSeek LLMå·²åˆå§‹åŒ–")

        # BM25ç´¢å¼•ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
        self.bm25 = None
        self.bm25_docs = None

        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

    def search_vector(self, query: str, top_k: int = 20) -> List[Dict]:
        """å‘é‡æ£€ç´¢ï¼ˆå¤šç´¢å¼•ï¼‰"""

        # å‘é‡åŒ–æŸ¥è¯¢
        query_embedding = self.encoder.encode(query, convert_to_numpy=True)
        query_embedding = query_embedding.astype('float32').reshape(1, -1)
        faiss.normalize_L2(query_embedding)

        all_results = []

        # æœç´¢æ‰€æœ‰ç´¢å¼•
        for i in range(NUM_INDICES):
            index_path = Path(f"{GDRIVE_PROJECT}/index_{i}")

            if not index_path.exists():
                continue

            # åŠ è½½ç´¢å¼•
            index = faiss.read_index(str(index_path / "faiss.index"))
            with open(index_path / "documents.pkl", 'rb') as f:
                data = pickle.load(f)

            # æœç´¢
            distances, indices = index.search(query_embedding, top_k // NUM_INDICES + 5)

            for dist, idx in zip(distances[0], indices[0]):
                if idx < len(data['documents']):
                    all_results.append({
                        'content': data['documents'][idx]['content'],
                        'metadata': data['documents'][idx]['metadata'],
                        'score': float(dist),
                        'source': 'vector'
                    })

        # æŒ‰åˆ†æ•°æ’åº
        all_results.sort(key=lambda x: x['score'], reverse=True)

        return all_results[:top_k]

    def search_bm25(self, query: str, top_k: int = 20) -> List[Dict]:
        """BM25æ£€ç´¢"""

        # å»¶è¿Ÿæ„å»ºBM25ç´¢å¼•
        if self.bm25 is None:
            print("ğŸ”¨ æ„å»ºBM25ç´¢å¼•ï¼ˆé¦–æ¬¡æŸ¥è¯¢éœ€è¦ï¼‰...")
            self._build_bm25_index()

        # åˆ†è¯
        query_tokens = re.findall(r'\b\w+\b', query.lower())

        # BM25è¯„åˆ†
        scores = self.bm25.get_scores(query_tokens)

        # è·å–Top-K
        top_indices = np.argsort(scores)[::-1][:top_k]

        results = []
        for idx in top_indices:
            if scores[idx] > 0:
                results.append({
                    'content': self.bm25_docs[idx]['content'],
                    'metadata': self.bm25_docs[idx]['metadata'],
                    'score': float(scores[idx]),
                    'source': 'bm25'
                })

        return results

    def _build_bm25_index(self):
        """æ„å»ºBM25ç´¢å¼•ï¼ˆä»æ‰€æœ‰å°ç´¢å¼•ä¸­åŠ è½½æ–‡æ¡£ï¼‰"""

        all_docs = []
        tokenized_corpus = []

        for i in range(NUM_INDICES):
            index_path = Path(f"{GDRIVE_PROJECT}/index_{i}")
            if not index_path.exists():
                continue

            with open(index_path / "documents.pkl", 'rb') as f:
                data = pickle.load(f)

            for doc in data['documents']:
                all_docs.append(doc)
                tokens = re.findall(r'\b\w+\b', doc['content'].lower())
                tokenized_corpus.append(tokens)

        self.bm25_docs = all_docs
        self.bm25 = BM25Okapi(tokenized_corpus)

        print(f"âœ“ BM25ç´¢å¼•å®Œæˆ: {len(all_docs):,} ä¸ªæ–‡æ¡£")

    def search_hybrid(
        self,
        query: str,
        top_k: int = 10,
        vector_weight: float = 0.7,
        use_rerank: bool = True
    ) -> List[Dict]:
        """
        æ··åˆæ£€ç´¢ï¼ˆå‘é‡ + BM25 + é‡æ’åºï¼‰

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›ç»“æœæ•°
            vector_weight: å‘é‡æ£€ç´¢æƒé‡ï¼ˆ0-1ï¼‰
            use_rerank: æ˜¯å¦ä½¿ç”¨é‡æ’åº
        """

        # 1. å‘é‡æ£€ç´¢
        vector_results = self.search_vector(query, top_k=top_k * 2)

        # 2. BM25æ£€ç´¢
        bm25_results = self.search_bm25(query, top_k=top_k * 2)

        # 3. èåˆï¼ˆå½’ä¸€åŒ–åˆ†æ•°ï¼‰
        def normalize_scores(results):
            if not results:
                return []
            scores = [r['score'] for r in results]
            min_s, max_s = min(scores), max(scores)
            range_s = max_s - min_s if max_s > min_s else 1.0

            for r in results:
                r['score'] = (r['score'] - min_s) / range_s
            return results

        vector_results = normalize_scores(vector_results)
        bm25_results = normalize_scores(bm25_results)

        # åˆå¹¶ï¼ˆåŠ æƒï¼‰
        merged = {}
        bm25_weight = 1.0 - vector_weight

        for r in vector_results:
            key = r['content'][:100]
            merged[key] = {
                'content': r['content'],
                'metadata': r['metadata'],
                'score': r['score'] * vector_weight,
                'source': 'hybrid'
            }

        for r in bm25_results:
            key = r['content'][:100]
            if key in merged:
                merged[key]['score'] += r['score'] * bm25_weight
            else:
                merged[key] = {
                    'content': r['content'],
                    'metadata': r['metadata'],
                    'score': r['score'] * bm25_weight,
                    'source': 'hybrid'
                }

        results = sorted(merged.values(), key=lambda x: x['score'], reverse=True)[:top_k * 2]

        # 4. é‡æ’åºï¼ˆå¯é€‰ï¼‰
        if use_rerank and self.reranker is not None:
            pairs = [[query, r['content'][:500]] for r in results]
            rerank_scores = self.reranker.predict(pairs)

            for r, score in zip(results, rerank_scores):
                r['score'] = float(score)
                r['source'] = 'hybrid_reranked'

            results.sort(key=lambda x: x['score'], reverse=True)

        return results[:top_k]

    def query_with_llm(
        self,
        query: str,
        top_k: int = 5,
        use_hybrid: bool = True
    ) -> tuple:
        """
        ä½¿ç”¨LLMç”Ÿæˆç­”æ¡ˆ

        Returns:
            (answer, retrieved_results)
        """

        if self.llm is None:
            raise ValueError("è¯·å…ˆè®¾ç½® deepseek_api_key")

        # æ£€ç´¢
        if use_hybrid:
            results = self.search_hybrid(query, top_k=top_k)
        else:
            results = self.search_vector(query, top_k=top_k)

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[æ–‡çŒ® {i+1}] ç›¸å…³åº¦: {r['score']:.3f}\n"
            f"å†…å®¹: {r['content'][:800]}"
            for i, r in enumerate(results[:5])
        ])

        # æ„å»ºPrompt
        prompt = f"""ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ–å­¦æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æ–‡çŒ®ç‰‡æ®µå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ç›´æ¥å›ç­”é—®é¢˜ï¼Œå¼•ç”¨å…·ä½“æ–‡çŒ®å†…å®¹
2. å¦‚æœæ–‡çŒ®ä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œè¯´æ˜ç›¸å…³ä¿¡æ¯
3. ä¿æŒå®¢è§‚å’Œä¸“ä¸š

æ–‡çŒ®ç‰‡æ®µï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""

        # è°ƒç”¨DeepSeek
        response = self.llm.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„åŒ–å­¦æ–‡çŒ®åˆ†æåŠ©æ‰‹ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=2048,
            temperature=0.7
        )

        answer = response.choices[0].message.content

        return answer, results


# ============ ä½¿ç”¨ç¤ºä¾‹ ============

def example_usage():
    """å®Œæ•´ä½¿ç”¨ç¤ºä¾‹"""

    print("="*80)
    print("ğŸš€ å¤šç´¢å¼•é«˜çº§RAGç³»ç»Ÿ")
    print("="*80)

    # 1. åˆå§‹åŒ–ï¼ˆä¸ä½¿ç”¨DeepSeekï¼‰
    rag = MultiIndexRAG(
        use_reranker=True,
        deepseek_api_key=None  # æš‚ä¸ä½¿ç”¨LLM
    )

    # 2. æµ‹è¯•ä¸åŒæ£€ç´¢ç­–ç•¥
    query = "Which photoinitiators work better under LED light sources?"

    print(f"\n{'='*80}")
    print(f"æŸ¥è¯¢: {query}")
    print(f"{'='*80}")

    # 2a. çº¯å‘é‡æ£€ç´¢
    print("\nã€ç­–ç•¥1ã€‘çº¯å‘é‡æ£€ç´¢:")
    results = rag.search_vector(query, top_k=3)
    for i, r in enumerate(results, 1):
        print(f"[{i}] {r['score']:.3f} | {r['content'][:100]}...")

    # 2b. æ··åˆæ£€ç´¢ + é‡æ’åº
    print("\nã€ç­–ç•¥2ã€‘æ··åˆæ£€ç´¢ + é‡æ’åº:")
    results = rag.search_hybrid(query, top_k=3)
    for i, r in enumerate(results, 1):
        print(f"[{i}] {r['score']:.3f} | {r['source']} | {r['content'][:100]}...")

    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


def query_with_deepseek():
    """ä½¿ç”¨DeepSeekç”Ÿæˆç­”æ¡ˆ"""

    # åˆå§‹åŒ–ï¼ˆå¸¦DeepSeekï¼‰
    rag = MultiIndexRAG(
        use_reranker=True,
        deepseek_api_key="sk-xxxxxxxxxxxxx"  # æ›¿æ¢ä¸ºä½ çš„APIå¯†é’¥
    )

    query = "Which photoinitiators work better under LED light sources?"

    # ç”Ÿæˆç­”æ¡ˆ
    answer, results = rag.query_with_llm(query, top_k=5, use_hybrid=True)

    print(f"\nğŸ’¡ DeepSeekå›ç­”:")
    print("="*80)
    print(answer)
    print("="*80)

    print(f"\nğŸ“š å‚è€ƒæ–‡çŒ®:")
    for i, r in enumerate(results, 1):
        print(f"[{i}] {r['score']:.3f} | {r['metadata']['doc_id'][:50]}")


# ============ è¿è¡Œ ============
if __name__ == "__main__":
    # å…ˆå®‰è£…ä¾èµ–
    # !pip install rank-bm25 openai -q

    # è¿è¡Œç¤ºä¾‹
    # example_usage()
    query_with_deepseek()

# ğŸ“ å®Œæ•´ä½¿ç”¨æµç¨‹ï¼ˆåœ¨Colabä¸­ï¼‰

# # ============================================
# # Cell 1: å®‰è£…é¢å¤–ä¾èµ–
# # ============================================
# !pip install rank-bm25 openai -q

# # ============================================
# # Cell 2: è¿è¡Œé«˜çº§RAGï¼ˆå¤åˆ¶ä¸Šé¢å®Œæ•´ä»£ç ï¼‰
# # ============================================
# # ï¼ˆç²˜è´´ä¸Šé¢çš„å®Œæ•´ä»£ç ï¼‰

# # ============================================
# # Cell 3: æµ‹è¯•ï¼ˆä¸ä½¿ç”¨DeepSeekï¼‰
# # ============================================
  # rag = MultiIndexRAG(use_reranker=True, deepseek_api_key=None)

  # # æ··åˆæ£€ç´¢æµ‹è¯•
  # results = rag.search_hybrid(
  #     "Which photoinitiators work better under LED?",
  #     top_k=5
  # )

  # for i, r in enumerate(results, 1):
  #     print(f"\n[{i}] åˆ†æ•°: {r['score']:.3f} | æ¥æº: {r['source']}")
  #     print(f"å†…å®¹: {r['content'][:200]}...")

# # ============================================
# # Cell 4: ä½¿ç”¨DeepSeekï¼ˆå¯é€‰ï¼‰
# # ============================================
  # rag = MultiIndexRAG(
  #     use_reranker=True,
  #     deepseek_api_key="sk-xxxxxxxxxx"  # å¡«å…¥ä½ çš„APIå¯†é’¥
  # )

  # answer, refs = rag.query_with_llm(
  #     "å¦‚ä½•é€‰æ‹©é€‚åˆLEDå…‰æºçš„å…‰å¼•å‘å‰‚ï¼Ÿ",
  #     top_k=5
  # )

  # print("ğŸ’¡ å›ç­”:")
  # print(answer)